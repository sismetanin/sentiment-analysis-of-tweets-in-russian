{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis of Tweets in Russian using Multinomial Naive Bayes\n",
    "Multinomial Naive Bayes classification algorithm tends to be a baseline solution for sentiment analysis task. The basic idea of Naïve Bayes technique is to find the probabilities of classes assigned to texts by using the joint probabilities of words and classes. Given the dependent feature vector $(x_1, \\dots, x_n)$ and the class $C_k$. Bayes' theorem is stated mathematically as the following relationship:\n",
    "$$\n",
    "\\begin{align}\n",
    "P(C_k \\mid x_1,\\dots,x_n) = \\frac{P(C_k)P(x_1,\\dots,x_n \\mid C_k)}{P(x_1,\\dots,x_n)}\n",
    "\\end{align}\n",
    "$$\n",
    "According to the \"naive\" conditional independence assumptions, for the given class $C_k$ each feature $x_{i}$ is conditionally independent of every other feature $x_{j}$ for$j\\neq i$.\n",
    "$$\n",
    "\\begin{align}\n",
    "P(x_i \\mid C_k,x_1,\\dots,x_n) = P(x_i \\mid C_k)\n",
    "\\end{align}\n",
    "$$\n",
    "Thus, the relation can be simplified to\n",
    "$$\n",
    "\\begin{align}\n",
    "P(C_k \\mid x_1,\\dots,x_n) = \\frac{P(C_k)\\prod_{i=1}^{n}P(x_i \\mid C_k)}{P(x_1,\\dots,x_n)}\n",
    "\\end{align}\n",
    "$$\n",
    "Since $P(x_1,...x_n)$ is constant, if the values of the feature variables are known, the following classification rule can be used:\n",
    "$$\n",
    "\\begin{align}\n",
    "P(C_k \\mid x_1,\\dots,x_n) \\propto  P(C_k)\\prod_{i=1}^{n}P(x_i \\mid C_k) \\\\ \\Downarrow \\\\ \\hat{y} = \\underset{k}{\\arg\\max} P(C_k)\\prod_{i=1}^{n}P(x_i \\mid C_k)\n",
    "\\end{align}\n",
    "$$\n",
    "To avoid underflow, log probabilities can be used.\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat{y} = \\underset{k}{\\arg\\max}(\\ln{P(C_k)}+\\sum_{i=1}^{n}\\ln{P(x_i \\mid C_k)})\n",
    "\\end{align}\n",
    "$$\n",
    "The variaty of naive Bayes classifiers primarly differs between each other by the assumptions they make regarding the distribution of $P(x_i \\mid C_k)$, while $P(C_k)$ is usually defined as the relative frequency of class $C_k$ in the training dataset.\n",
    "\n",
    "The multinomial distribution is parametrized by vectors $\\theta_k = (\\theta_{k1},\\ldots,\\theta_{kn})$ for each class $C_k$, where $n$ is the number of features (i.e. the size of the vocabulary) and $\\theta_{ki}$ is the probability $P(x_i \\mid C_k)$ of feature $i$ appearing in a sample that belongs to the class $C_k$.\n",
    "\n",
    "The parameters $\\theta_y$ is estimated by a smoothed version of maximum likelihood, i.e. relative frequency counting:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat{\\theta}_{ki} = \\frac{ N_{ki} + \\alpha}{N_k + \\alpha n}\n",
    "\\end{align}\n",
    "$$\n",
    "where $N_{ki} = \\sum_{x \\in T} x_i$ is the number of times feature $i$ appears in a sample of class y in the training set $T$, and $N_{y} = \\sum_{i=1}^{|T|} N_{ki}$ is the total count of all features for class $C_k$. The smoothing priors $\\alpha \\ge 0$ accounts for features not present in the learning samples and prevents zero probabilities in further computations. Setting $\\alpha = 1$ is called Laplace smoothing, while $\\alpha < 1$ is called Lidstone smoothing.\n",
    "\n",
    "Thus, the final decision rule is defined as follows:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat{y} = \\underset{k}{\\arg\\max}(\\ln{P(C_k)}+\\sum_{i=1}^{n}\\ln{\\frac{ N_{ki} + \\alpha}{N_k + \\alpha n}})\n",
    "\\end{align}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading data\n",
    "Sentiment dataset of Tweets in Russian [1] is avaialble at http://study.mokoron.com/. The files positive.csv and negative.csv contain positively labelled and negatively labelled tweets, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data from CSV files\n",
    "n = ['id', 'date','name','text','typr','rep','rtw','faw','stcount','foll','frien','listcount']\n",
    "data_positive = pd.read_csv('positive.csv', sep=';',error_bad_lines=False, names=n, usecols=['text'])\n",
    "data_negative = pd.read_csv('negative.csv', sep=';',error_bad_lines=False, names=n, usecols=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create balanced dataset\n",
    "sample_size = min(data_positive.shape[0], data_negative.shape[0])\n",
    "raw_data = np.concatenate((data_positive['text'].values[:sample_size], \n",
    "                           data_negative['text'].values[:sample_size]), axis=0) \n",
    "labels = [1]*sample_size + [0]*sample_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing data\n",
    "Texts generated by humans in social media sites contain lots of noise that can significantly affect the results of the sentiment classification process. Moreover, depending on the features generation approach, every new term seems to add at least one new dimension to the feature space. That makes the feature space more sparse and high-dimensional. Consequently, the task of the classifier has become more complex.\n",
    "\n",
    "To prepare messages, such text preprocessing techniques as replacing URLs and usernames with keywords, removing punctuation marks and converting to lowercase were used in this program. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','URL', text)\n",
    "    text = re.sub('@[^\\s]+','USER', text)\n",
    "    text = text.lower().replace(\"ё\", \"е\")\n",
    "    text = re.sub('[^a-zA-Zа-яА-Я1-9]+', ' ', text)\n",
    "    text = re.sub(' +',' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "data = [preprocess_text(t) for t in raw_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training MultinomialNB\n",
    "A Pipeline class was used to make the vectorizer => transformer => classifier easier to work with. Such hyper-parameters as n-grams range, IDF usage, TF-IDF normalization type and Naive Bayes alpha were tunned using grid search. The performance of the selected hyper-parameters was measured on a test set that was not used during the model training step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', MultinomialNB())])\n",
    "\n",
    "tuned_parameters = {\n",
    "    'vect__ngram_range': [(1, 1), (1, 2), (2, 2)],\n",
    "    'tfidf__use_idf': (True, False),\n",
    "    'tfidf__norm': ('l1', 'l2'),\n",
    "    'clf__alpha': [1, 1e-1, 1e-2]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset was plited into train and test subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tuning hyper-parameters for f1_macro\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'clf__alpha': 1, 'tfidf__norm': 'l2', 'tfidf__use_idf': True, 'vect__ngram_range': (1, 2)}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.732 (+/-0.006) for {'clf__alpha': 1, 'tfidf__norm': 'l1', 'tfidf__use_idf': True, 'vect__ngram_range': (1, 1)}\n",
      "0.741 (+/-0.006) for {'clf__alpha': 1, 'tfidf__norm': 'l1', 'tfidf__use_idf': True, 'vect__ngram_range': (1, 2)}\n",
      "0.705 (+/-0.008) for {'clf__alpha': 1, 'tfidf__norm': 'l1', 'tfidf__use_idf': True, 'vect__ngram_range': (2, 2)}\n",
      "0.728 (+/-0.004) for {'clf__alpha': 1, 'tfidf__norm': 'l1', 'tfidf__use_idf': False, 'vect__ngram_range': (1, 1)}\n",
      "0.730 (+/-0.005) for {'clf__alpha': 1, 'tfidf__norm': 'l1', 'tfidf__use_idf': False, 'vect__ngram_range': (1, 2)}\n",
      "0.702 (+/-0.006) for {'clf__alpha': 1, 'tfidf__norm': 'l1', 'tfidf__use_idf': False, 'vect__ngram_range': (2, 2)}\n",
      "0.734 (+/-0.006) for {'clf__alpha': 1, 'tfidf__norm': 'l2', 'tfidf__use_idf': True, 'vect__ngram_range': (1, 1)}\n",
      "0.753 (+/-0.006) for {'clf__alpha': 1, 'tfidf__norm': 'l2', 'tfidf__use_idf': True, 'vect__ngram_range': (1, 2)}\n",
      "0.711 (+/-0.007) for {'clf__alpha': 1, 'tfidf__norm': 'l2', 'tfidf__use_idf': True, 'vect__ngram_range': (2, 2)}\n",
      "0.738 (+/-0.004) for {'clf__alpha': 1, 'tfidf__norm': 'l2', 'tfidf__use_idf': False, 'vect__ngram_range': (1, 1)}\n",
      "0.750 (+/-0.005) for {'clf__alpha': 1, 'tfidf__norm': 'l2', 'tfidf__use_idf': False, 'vect__ngram_range': (1, 2)}\n",
      "0.712 (+/-0.008) for {'clf__alpha': 1, 'tfidf__norm': 'l2', 'tfidf__use_idf': False, 'vect__ngram_range': (2, 2)}\n",
      "0.726 (+/-0.005) for {'clf__alpha': 0.1, 'tfidf__norm': 'l1', 'tfidf__use_idf': True, 'vect__ngram_range': (1, 1)}\n",
      "0.748 (+/-0.008) for {'clf__alpha': 0.1, 'tfidf__norm': 'l1', 'tfidf__use_idf': True, 'vect__ngram_range': (1, 2)}\n",
      "0.707 (+/-0.009) for {'clf__alpha': 0.1, 'tfidf__norm': 'l1', 'tfidf__use_idf': True, 'vect__ngram_range': (2, 2)}\n",
      "0.738 (+/-0.005) for {'clf__alpha': 0.1, 'tfidf__norm': 'l1', 'tfidf__use_idf': False, 'vect__ngram_range': (1, 1)}\n",
      "0.752 (+/-0.006) for {'clf__alpha': 0.1, 'tfidf__norm': 'l1', 'tfidf__use_idf': False, 'vect__ngram_range': (1, 2)}\n",
      "0.711 (+/-0.008) for {'clf__alpha': 0.1, 'tfidf__norm': 'l1', 'tfidf__use_idf': False, 'vect__ngram_range': (2, 2)}\n",
      "0.717 (+/-0.006) for {'clf__alpha': 0.1, 'tfidf__norm': 'l2', 'tfidf__use_idf': True, 'vect__ngram_range': (1, 1)}\n",
      "0.737 (+/-0.007) for {'clf__alpha': 0.1, 'tfidf__norm': 'l2', 'tfidf__use_idf': True, 'vect__ngram_range': (1, 2)}\n",
      "0.699 (+/-0.008) for {'clf__alpha': 0.1, 'tfidf__norm': 'l2', 'tfidf__use_idf': True, 'vect__ngram_range': (2, 2)}\n",
      "0.734 (+/-0.005) for {'clf__alpha': 0.1, 'tfidf__norm': 'l2', 'tfidf__use_idf': False, 'vect__ngram_range': (1, 1)}\n",
      "0.750 (+/-0.007) for {'clf__alpha': 0.1, 'tfidf__norm': 'l2', 'tfidf__use_idf': False, 'vect__ngram_range': (1, 2)}\n",
      "0.707 (+/-0.007) for {'clf__alpha': 0.1, 'tfidf__norm': 'l2', 'tfidf__use_idf': False, 'vect__ngram_range': (2, 2)}\n",
      "0.710 (+/-0.006) for {'clf__alpha': 0.01, 'tfidf__norm': 'l1', 'tfidf__use_idf': True, 'vect__ngram_range': (1, 1)}\n",
      "0.729 (+/-0.008) for {'clf__alpha': 0.01, 'tfidf__norm': 'l1', 'tfidf__use_idf': True, 'vect__ngram_range': (1, 2)}\n",
      "0.692 (+/-0.008) for {'clf__alpha': 0.01, 'tfidf__norm': 'l1', 'tfidf__use_idf': True, 'vect__ngram_range': (2, 2)}\n",
      "0.727 (+/-0.006) for {'clf__alpha': 0.01, 'tfidf__norm': 'l1', 'tfidf__use_idf': False, 'vect__ngram_range': (1, 1)}\n",
      "0.744 (+/-0.007) for {'clf__alpha': 0.01, 'tfidf__norm': 'l1', 'tfidf__use_idf': False, 'vect__ngram_range': (1, 2)}\n",
      "0.700 (+/-0.008) for {'clf__alpha': 0.01, 'tfidf__norm': 'l1', 'tfidf__use_idf': False, 'vect__ngram_range': (2, 2)}\n",
      "0.706 (+/-0.005) for {'clf__alpha': 0.01, 'tfidf__norm': 'l2', 'tfidf__use_idf': True, 'vect__ngram_range': (1, 1)}\n",
      "0.715 (+/-0.007) for {'clf__alpha': 0.01, 'tfidf__norm': 'l2', 'tfidf__use_idf': True, 'vect__ngram_range': (1, 2)}\n",
      "0.686 (+/-0.009) for {'clf__alpha': 0.01, 'tfidf__norm': 'l2', 'tfidf__use_idf': True, 'vect__ngram_range': (2, 2)}\n",
      "0.721 (+/-0.006) for {'clf__alpha': 0.01, 'tfidf__norm': 'l2', 'tfidf__use_idf': False, 'vect__ngram_range': (1, 1)}\n",
      "0.731 (+/-0.006) for {'clf__alpha': 0.01, 'tfidf__norm': 'l2', 'tfidf__use_idf': False, 'vect__ngram_range': (1, 2)}\n",
      "0.694 (+/-0.008) for {'clf__alpha': 0.01, 'tfidf__norm': 'l2', 'tfidf__use_idf': False, 'vect__ngram_range': (2, 2)}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.7397    0.7941    0.7659     37078\n",
      "          1     0.7759    0.7183    0.7460     36792\n",
      "\n",
      "avg / total     0.7577    0.7564    0.7560     73870\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "score = 'f1_macro'\n",
    "print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "print()\n",
    "np.errstate(divide='ignore')\n",
    "clf = GridSearchCV(text_clf, tuned_parameters, cv=10, scoring=score)\n",
    "clf.fit(x_train, y_train)\n",
    "\n",
    "print(\"Best parameters set found on development set:\")\n",
    "print()\n",
    "print(clf.best_params_)\n",
    "print()\n",
    "print(\"Grid scores on development set:\")\n",
    "print()\n",
    "for mean, std, params in zip(clf.cv_results_['mean_test_score'], \n",
    "                             clf.cv_results_['std_test_score'], \n",
    "                             clf.cv_results_['params']):\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))\n",
    "print()\n",
    "\n",
    "print(\"Detailed classification report:\")\n",
    "print()\n",
    "print(\"The model is trained on the full development set.\")\n",
    "print(\"The scores are computed on the full evaluation set.\")\n",
    "print()\n",
    "print(classification_report(y_test, clf.predict(x_test), digits=4))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Conslusion\n",
    "The model, which was trained on the development set, demonstrated $F_1=0.765$ on the evaluation set.\n",
    "\n",
    "## References\n",
    "1. Y. Rubtsova, \"Constructing a Corpus for Sentiment Classification Training\", Software & Systems, vol. 109, no. 1, pp. 72-78, 2015. \n",
    "2. \"Naive Bayes\", scikit-learn.org, 2018. [Online]. Available: http://scikit-learn.org/stable/modules/naive_bayes.html. [Accessed: 26- Aug- 2018]. \n",
    "3. \"Working With Text Data\", scikit-learn.org, 2018. [Online]. Available: http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html. [Accessed: 26- Aug- 2018]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
